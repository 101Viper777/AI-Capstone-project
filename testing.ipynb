{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying out with gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "api_key = \"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\"  # Replace with your key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def predict(message, history):\n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "  \n",
    "    response = client.chat.completions.create(model='gpt-3.5-turbo',\n",
    "    messages= history_openai_format,\n",
    "    temperature=1.0,\n",
    "    stream=True)\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "              partial_message = partial_message + chunk.choices[0].delta.content\n",
    "              yield partial_message\n",
    "\n",
    "gr.ChatInterface(predict).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "MultimodalTextbox expects a dictionary with optional keys 'text' and 'files'. Received str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m partial_message\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# The Gradio interface setup with multimodal capability\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m demo \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatInterface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhola\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerhaba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultimodal Chatbot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimodal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m demo\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "File \u001b[0;32m~/Documents/GitHub/AI-Capstone-project/env/lib/python3.11/site-packages/gradio/chat_interface.py:280\u001b[0m, in \u001b[0;36mChatInterface.__init__\u001b[0;34m(self, fn, multimodal, chatbot, textbox, additional_inputs, additional_inputs_accordion_name, additional_inputs_accordion, examples, cache_examples, title, description, theme, css, js, head, analytics_enabled, submit_btn, stop_btn, retry_btn, undo_btn, clear_btn, autofocus, concurrency_limit, fill_height, delete_cache)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m         examples_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_examples_fn\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples_handler \u001b[38;5;241m=\u001b[39m \u001b[43mExamples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextbox\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchatbot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m any_unrendered_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mis_rendered \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_inputs\n\u001b[1;32m    289\u001b[0m )\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_inputs \u001b[38;5;129;01mand\u001b[39;00m any_unrendered_inputs:\n",
      "File \u001b[0;32m~/Documents/GitHub/AI-Capstone-project/env/lib/python3.11/site-packages/gradio/helpers.py:54\u001b[0m, in \u001b[0;36mcreate_examples\u001b[0;34m(examples, inputs, outputs, fn, cache_examples, examples_per_page, _api_mode, label, elem_id, run_on_click, preprocess, postprocess, api_name, batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_examples\u001b[39m(\n\u001b[1;32m     38\u001b[0m     examples: \u001b[38;5;28mlist\u001b[39m[Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Any]] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     39\u001b[0m     inputs: Component \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[Component],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     batch: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m ):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Top-level synchronous function that creates Examples. Provided for backwards compatibility, i.e. so that gr.Examples(...) can be used to create the Examples component.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     examples_obj \u001b[38;5;241m=\u001b[39m \u001b[43mExamples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples_per_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples_per_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_api_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_api_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43melem_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melem_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_on_click\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_on_click\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_initiated_directly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     examples_obj\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples_obj\n",
      "File \u001b[0;32m~/Documents/GitHub/AI-Capstone-project/env/lib/python3.11/site-packages/gradio/helpers.py:209\u001b[0m, in \u001b[0;36mExamples.__init__\u001b[0;34m(self, examples, inputs, outputs, fn, cache_examples, examples_per_page, _api_mode, label, elem_id, run_on_click, preprocess, postprocess, api_name, batch, _initiated_directly)\u001b[0m\n\u001b[1;32m    207\u001b[0m sub \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m component, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, example):\n\u001b[0;32m--> 209\u001b[0m     prediction_value \u001b[38;5;241m=\u001b[39m \u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prediction_value, (GradioRootModel, GradioModel)):\n\u001b[1;32m    211\u001b[0m         prediction_value \u001b[38;5;241m=\u001b[39m prediction_value\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/Documents/GitHub/AI-Capstone-project/env/lib/python3.11/site-packages/gradio/components/multimodal_textbox.py:160\u001b[0m, in \u001b[0;36mMultimodalTextbox.postprocess\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MultimodalData(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, files\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultimodalTextbox expects a dictionary with optional keys \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m text \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m value \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: MultimodalTextbox expects a dictionary with optional keys 'text' and 'files'. Received str"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"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\"  # Replace with your key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def predict(x, history):\n",
    "    # Extracting text and file from the multimodal input\n",
    "    text = x['text']\n",
    "    file = x['file']\n",
    "    \n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human})\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    \n",
    "    # Adding the current message to the history format\n",
    "    if text:  # Check if there's text input\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": text})\n",
    "    \n",
    "    if file is not None:  # Check if there's a file input\n",
    "        # Here, you would handle the file (e.g., encode it or upload it to a location where it can be referenced)\n",
    "        # For this example, let's assume we're encoding or handling the file somehow\n",
    "        file_data = \"encoded_or_handled_file_data_here\"\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": file_data})\n",
    "\n",
    "    # Call to the OpenAI API with multimodal input\n",
    "    # Note: Adjust the 'model' parameter as needed based on the capabilities at your disposal\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo',  # Assuming GPT-4 can handle multimodal inputs; replace with the correct model name\n",
    "        messages=history_openai_format,\n",
    "        temperature=1.0,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            partial_message += chunk.choices[0].delta.content\n",
    "            yield partial_message\n",
    "\n",
    "# The Gradio interface setup with multimodal capability\n",
    "demo = gr.ChatInterface(\n",
    "    fn=predict, \n",
    "    examples=[\"hello\", \"hola\", \"merhaba\"], \n",
    "    title=\"Multimodal Chatbot\",\n",
    "    multimodal=True\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dresify",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
